# 🏥 Medical Visual Question Answering (VQA) with IDEFICS & LLaVA

![Python](https://img.shields.io/badge/Python-3.x-blue) ![LLM](https://img.shields.io/badge/LLM-Multimodal-green) ![Kaggle](https://img.shields.io/badge/Kaggle-Training-blue)

A **Medical Visual Question Answering (VQA) system** using **fine-tuned IDEFICS and LLaVA models** to answer questions based on medical images. 

## 🚀 Features
- 🏥 Fine-tuned **IDEFICS** and **LLaVA** models for medical VQA
- 🎯 Achieved **42% accuracy** with IDEFICS and **38% accuracy** with LLaVA
- 💻 Trained on **Kaggle using a P100 GPU**
- 📂 Provides **pre-trained models** for inference

## 📑 Table of Contents
- [Tech Stack](#-tech-stack)
- [Training Process](#-training-process)
- [Usage](#-usage)
- [Demo](#-demo)
- [Contributing](#-contributing)
- [License](#-license)
- [Author](#-author)

## 🛠 Tech Stack
- **Model:** IDEFICS & LLaVA (fine-tuned for medical VQA)
- **Training Environment:** Kaggle (P100 GPU)
- **Dataset:** PathVQA

## 🎓 Training Process
1. **Dataset Preparation:** Loaded and preprocessed the **PathVQA** dataset.
2. **Model Fine-Tuning:**
   - Trained **IDEFICS** and **LLaVA** separately.
   - Used **Kaggle's P100 GPU** for computation.
   - Achieved **42% accuracy** on IDEFICS and **38% accuracy** on LLaVA.
3. **Model Saving:** Exported trained models for inference.

## 🎯 Usage
- The trained models can be used for **medical image-based question answering**.
- Simply load the models and pass medical images with corresponding questions for predictions.

## 📸 Demo
![Medical VQA Model Output](path_to_screenshot.png)

## 🤝 Contributing
Contributions are welcome! Open an issue or submit a pull request to improve the project.

## 📜 License
This project is open-source under the MIT License.

## 👤 Author
Developed by Hari nath for advancing **medical AI research**.

---
🔥 If you like this project, give it a ⭐ on GitHub! 🚀

