# ğŸ¥ Medical Visual Question Answering (VQA) with IDEFICS & LLaVA

![Python](https://img.shields.io/badge/Python-3.x-blue) ![LLM](https://img.shields.io/badge/LLM-Multimodal-green) ![Kaggle](https://img.shields.io/badge/Kaggle-Training-blue)

A **Medical Visual Question Answering (VQA) system** using **fine-tuned IDEFICS and LLaVA models** to answer questions based on medical images. 

## ğŸš€ Features
- ğŸ¥ Fine-tuned **IDEFICS** and **LLaVA** models for medical VQA
- ğŸ¯ Achieved **42% accuracy** with IDEFICS and **38% accuracy** with LLaVA
- ğŸ’» Trained on **Kaggle using a P100 GPU**
- ğŸ“‚ Provides **pre-trained models** for inference

## ğŸ“‘ Table of Contents
- [Tech Stack](#-tech-stack)
- [Training Process](#-training-process)
- [Usage](#-usage)
- [Demo](#-demo)
- [Contributing](#-contributing)
- [License](#-license)
- [Author](#-author)

## ğŸ›  Tech Stack
- **Model:** IDEFICS & LLaVA (fine-tuned for medical VQA)
- **Training Environment:** Kaggle (P100 GPU)
- **Dataset:** PathVQA

## ğŸ“ Training Process
1. **Dataset Preparation:** Loaded and preprocessed the **PathVQA** dataset.
2. **Model Fine-Tuning:**
   - Trained **IDEFICS** and **LLaVA** separately.
   - Used **Kaggle's P100 GPU** for computation.
   - Achieved **42% accuracy** on IDEFICS and **38% accuracy** on LLaVA.
3. **Model Saving:** Exported trained models for inference.

## ğŸ¯ Usage
- The trained models can be used for **medical image-based question answering**.
- Simply load the models and pass medical images with corresponding questions for predictions.

## ğŸ“¸ Demo
![Medical VQA Model Output](path_to_screenshot.png)

## ğŸ¤ Contributing
Contributions are welcome! Open an issue or submit a pull request to improve the project.

## ğŸ“œ License
This project is open-source under the MIT License.

## ğŸ‘¤ Author
Developed by Hari nath for advancing **medical AI research**.

---
ğŸ”¥ If you like this project, give it a â­ on GitHub! ğŸš€

